"""
COMPLETE INTEGRATION PIPELINE - FINAL VERSION WITH VULNERABILITY MODEL
‚úÖ FIXED: Proper model loading with size mismatch handling
‚úÖ FIXED: No Hugging Face downloads - only loads from local models/
‚úÖ FIXED: Explicit model status tracking
‚úÖ FIXED: Fail-fast if model is missing
"""

import os
import sys
import json
import time
import traceback
import torch
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from transformers import RobertaTokenizer, RobertaForSequenceClassification

# ================== FIXED PATH CONFIGURATION ==================
# ‚úÖ REQUIRED FIX: Use relative paths based on current file location
project_root = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, "src"))

print(f"üîß Project root: {project_root}")
print(f"üîß Src path: {os.path.join(project_root, 'src')}")

# Import all modules with error handling
COMPONENTS = {}

# ================== LOAD ALL COMPONENTS ==================

print("üîß Loading all components...")

# Language Detector
try:
    from language_detector import get_language_detector
    COMPONENTS['language_detector'] = get_language_detector()
    print("‚úÖ Language detector loaded")
except ImportError as e:
    print(f"‚ùå Language detector import failed: {e}")
    COMPONENTS['language_detector'] = None

# Style Checker
try:
    from style_checker import get_style_checker
    COMPONENTS['style_checker'] = get_style_checker()
    print("‚úÖ Style checker loaded")
except ImportError as e:
    print(f"‚ùå Style checker import failed: {e}")
    COMPONENTS['style_checker'] = None

# Security Scanner
try:
    from security_scanner import get_security_scanner
    COMPONENTS['security_scanner'] = get_security_scanner()
    print("‚úÖ Security scanner loaded")
except ImportError as e:
    print(f"‚ùå Security scanner import failed: {e}")
    COMPONENTS['security_scanner'] = None

# AUG-PDG
try:
    from aug_pdg import Pipeline as AugPDGPipeline
    COMPONENTS['aug_pdg_builder'] = AugPDGPipeline
    print("‚úÖ AUG-PDG loaded")
except ImportError as e:
    print(f"‚ùå AUG-PDG import failed: {e}")
    COMPONENTS['aug_pdg_builder'] = None

# ================== VULNERABILITY MODEL LOADER ==================

class ModelLoadError(Exception):
    """Exception raised when model fails to load."""
    pass

class VulnerabilityModelLoader:
    """
    Loads vulnerability detection model
    ‚úÖ RULE 1: Only loads from local models/ directory
    ‚úÖ RULE 2: NEVER downloads from Hugging Face
    ‚úÖ RULE 3: Properly handles size mismatches
    ‚úÖ RULE 4: Explicit status tracking
    ‚úÖ RULE 5: Fail-fast if model missing
    """
    
    LOCAL_MODEL_PATH = os.path.join(project_root, "models", "vulnerability_logic_production")

    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.status = "UNINITIALIZED"
        self.load_model()

    def load_model(self):
        """
        Load model ONLY from local directory
        Raises ModelLoadError if model cannot be loaded
        """
        # -------------------------------
        # ‚úÖ FIXED: Only check local model path
        # -------------------------------
        print(f"üì¶ Loading vulnerability model from LOCAL path: {self.LOCAL_MODEL_PATH}")
        
        # Check if local model directory exists
        if not os.path.exists(self.LOCAL_MODEL_PATH):
            self.status = "MODEL_MISSING"
            raise ModelLoadError(
                f"Model directory not found: {self.LOCAL_MODEL_PATH}\n"
                "Please run: python scripts/download_model.py"
            )
        
        # Check for required files
        config_path = os.path.join(self.LOCAL_MODEL_PATH, "config.json")
        if not os.path.exists(config_path):
            self.status = "INCOMPLETE_MODEL"
            raise ModelLoadError(
                f"Model config not found: {config_path}\n"
                "Model download may be incomplete."
            )
        
        # Check for model weights
        safetensors_path = os.path.join(self.LOCAL_MODEL_PATH, "model.safetensors")
        pytorch_path = os.path.join(self.LOCAL_MODEL_PATH, "pytorch_model.bin")
        
        if not os.path.exists(safetensors_path) and not os.path.exists(pytorch_path):
            self.status = "NO_MODEL_WEIGHTS"
            raise ModelLoadError(
                f"No model weights found in: {self.LOCAL_MODEL_PATH}\n"
                "Expected either model.safetensors or pytorch_model.bin"
            )
        
        try:
            # -------------------------------
            # ‚úÖ FIXED: Load tokenizer from local path
            # -------------------------------
            print(f"   Loading tokenizer...")
            self.tokenizer = RobertaTokenizer.from_pretrained(self.LOCAL_MODEL_PATH)
            
            # Get tokenizer vocabulary size
            vocab_size = len(self.tokenizer)
            print(f"   Tokenizer vocab size: {vocab_size}")
            
            # -------------------------------
            # ‚úÖ FIXED: Load model with ignore_mismatched_sizes=True
            # This is REQUIRED when the pre-trained model's vocab size
            # doesn't match the saved model's vocab size
            # -------------------------------
            print(f"   Loading model weights...")
            
            # Try safetensors first, then pytorch
            if os.path.exists(safetensors_path):
                print(f"   Using safetensors format")
                self.model = RobertaForSequenceClassification.from_pretrained(
                    self.LOCAL_MODEL_PATH,
                    use_safetensors=True,
                    ignore_mismatched_sizes=True  # ‚úÖ CRITICAL FIX: Allow size mismatch
                )
            else:
                print(f"   Using pytorch format")
                self.model = RobertaForSequenceClassification.from_pretrained(
                    self.LOCAL_MODEL_PATH,
                    ignore_mismatched_sizes=True  # ‚úÖ CRITICAL FIX: Allow size mismatch
                )
            
            # Move to device
            self.model.to(self.device)
            self.model.eval()
            
            # Verify model loaded correctly
            embedding_size = self.model.config.vocab_size
            print(f"   Model vocab size: {embedding_size}")
            
            # Note: With ignore_mismatched_sizes=True, the model's embedding layer
            # will be automatically resized to match the tokenizer's vocab size
            # This is expected behavior when loading fine-tuned models
            
            self.status = "MODEL_LOADED"
            print(f"‚úÖ Vulnerability model loaded successfully on {self.device}")
            print(f"   Status: {self.status}")
            print(f"   Classifier labels: {self.model.config.id2label}")
            
        except Exception as e:
            self.status = "LOAD_FAILED"
            raise ModelLoadError(
                f"Failed to load model from {self.LOCAL_MODEL_PATH}: {str(e)}\n"
                f"Traceback: {traceback.format_exc()}"
            ) from e

    def predict_vulnerability(self, source, sink, sanitization):
        """
        Predict if a code pattern is vulnerable
        Uses the trained CodeBERT model
        """
        # Check model status
        if self.status != "MODEL_LOADED" or not self.model or not self.tokenizer:
            raise RuntimeError(
                f"Cannot make predictions: model status is '{self.status}'\n"
                "Model must be successfully loaded before making predictions."
            )

        # Format input as trained
        input_text = f"""[VULNERABILITY_FLOW]
SOURCE: {source}
SINK: {sink}
SANITIZATION: {sanitization}"""

        # Tokenize
        inputs = self.tokenizer(
            input_text,
            truncation=True,
            padding=True,
            max_length=128,
            return_tensors="pt"
        )

        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # Predict
        with torch.no_grad():
            outputs = self.model(**inputs)
            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
            predicted_class = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][predicted_class].item()

        # Get label mapping
        label_map = self.model.config.id2label
        class_name = label_map.get(predicted_class, f"class_{predicted_class}")

        return {
            "is_vulnerable": bool(predicted_class == 1),
            "predicted_class": predicted_class,
            "predicted_class_name": class_name,
            "confidence": confidence,
            "vulnerability_score": probabilities[0][1].item() if len(probabilities[0]) > 1 else 0.0,
            "safe_score": probabilities[0][0].item() if len(probabilities[0]) > 0 else 0.0,
            "model_status": self.status,
            "device": str(self.device)
        }
    
    def get_status(self):
        """Get current model status."""
        return {
            "status": self.status,
            "model_loaded": self.model is not None,
            "tokenizer_loaded": self.tokenizer is not None,
            "device": str(self.device),
            "model_path": self.LOCAL_MODEL_PATH,
            "vocab_size": len(self.tokenizer) if self.tokenizer else 0,
            "model_vocab_size": self.model.config.vocab_size if self.model else 0
        }

# ================== PIPELINE CLASSES ==================

class PipelineStatus(Enum):
    """Status of pipeline analysis."""
    SUCCESS = "success"
    PARTIAL = "partial"
    ERROR = "error"
    SKIPPED = "skipped"

@dataclass
class ComponentResult:
    """Result from a single component."""
    name: str
    status: PipelineStatus
    data: Dict[str, Any]
    error: Optional[str] = None
    processing_time: float = 0.0

@dataclass
class PipelineResult:
    """Complete pipeline analysis results."""
    filename: str
    language: str

    # Component results
    language_detection: ComponentResult
    style_analysis: ComponentResult
    security_analysis: ComponentResult
    aug_pdg_analysis: Optional[ComponentResult] = None
    vulnerability_prediction: Optional[ComponentResult] = None

    # Combined metrics
    overall_score: int = 0
    overall_status: str = "UNKNOWN"
    final_decision: str = "UNKNOWN"
    total_processing_time: float = 0.0
    timestamp: float = 0.0

    # Recommendations
    recommendations: List[str] = None

    def __post_init__(self):
        if self.recommendations is None:
            self.recommendations = []

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        result = asdict(self)
        result['language_detection']['status'] = self.language_detection.status.value
        result['style_analysis']['status'] = self.style_analysis.status.value
        result['security_analysis']['status'] = self.security_analysis.status.value

        if self.aug_pdg_analysis:
            result['aug_pdg_analysis']['status'] = self.aug_pdg_analysis.status.value
        if self.vulnerability_prediction:
            result['vulnerability_prediction']['status'] = self.vulnerability_prediction.status.value

        return result

class CodeReviewPipeline:
    """
    Main pipeline for complete code review
    """

    def __init__(self):
        self.components = COMPONENTS
        self.supported_languages = ["python", "java", "javascript", "php", "ruby", "go"]
        
        # Confidence threshold for language detection
        self.language_confidence_threshold = 0.7  # 70%
        
        # Initialize vulnerability model (may raise ModelLoadError)
        print(f"\nü§ñ Initializing vulnerability model...")
        try:
            self.vulnerability_model = VulnerabilityModelLoader()
            model_status = self.vulnerability_model.get_status()
            print(f"   Model status: {model_status['status']}")
            print(f"   Vocab sizes - Tokenizer: {model_status.get('vocab_size', 'N/A')}, Model: {model_status.get('model_vocab_size', 'N/A')}")
        except ModelLoadError as e:
            print(f"‚ùå CRITICAL: Vulnerability model failed to load")
            print(f"   Error: {e}")
            print(f"   Please ensure model is downloaded: python scripts/download_model.py")
            raise  # Re-raise to fail-fast

        print(f"\nüìã Available Components:")
        for name, component in self.components.items():
            status = "‚úÖ" if component is not None else "‚ùå"
            print(f"  {status} {name}")
        
        # Show model status
        model_status = self.vulnerability_model.get_status()
        status_symbol = "‚úÖ" if model_status['status'] == "MODEL_LOADED" else "‚ùå"
        print(f"  {status_symbol} vulnerability_model [{model_status['status']}]")

    def process_file(self, filepath: str) -> PipelineResult:
        """
        Process a single file through the complete pipeline
        """
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"File not found: {filepath}")

        with open(filepath, 'r', encoding='utf-8') as f:
            code = f.read()

        filename = os.path.basename(filepath)

        return self.analyze_code(code, filename)

    def analyze_code(self, code: str, filename: str = "unknown.txt") -> PipelineResult:
        """
        Complete analysis pipeline for code
        """
        start_time = time.time()
        component_results = {}
        recommendations = []

        print(f"\n{'='*70}")
        print(f"üöÄ ANALYZING: {filename}")
        print(f"{'='*70}")

        # ================== STEP 1: LANGUAGE DETECTION ==================
        print(f"\n[1/5] üîç Language Detection")
        lang_detection_time = time.time()

        language = "unknown"
        language_supported = False
        language_confidence = 0.0

        if self.components['language_detector']:
            try:
                detector = self.components['language_detector']
                language, confidence = detector.detect(filename, code)
                language_confidence = confidence
                language_supported = language and language.lower() in self.supported_languages

                component_results['language_detection'] = ComponentResult(
                    name="language_detector",
                    status=PipelineStatus.SUCCESS,
                    data={
                        "language": language or "unknown",
                        "confidence": confidence,
                        "supported": language_supported
                    },
                    processing_time=time.time() - lang_detection_time
                )

                print(f"   ‚Üí Detected: {language or 'unknown'} (confidence: {confidence:.1%})")
                print(f"   ‚Üí Supported: {'‚úÖ Yes' if language_supported else '‚ùå No'}")

                # Check confidence threshold
                if confidence < self.language_confidence_threshold:
                    print(f"   ‚ö†Ô∏è  Low confidence detection (<{self.language_confidence_threshold:.0%})")
                    component_results['language_detection'].data["supported"] = False

            except Exception as e:
                component_results['language_detection'] = ComponentResult(
                    name="language_detector",
                    status=PipelineStatus.ERROR,
                    data={"language": "unknown", "confidence": 0.0, "supported": False},
                    error=str(e),
                    processing_time=time.time() - lang_detection_time
                )
                print(f"   ‚Üí Failed: {e}")
        else:
            language = self._detect_by_extension(filename)
            language_confidence = 0.7  # Default confidence for extension detection
            language_supported = language in self.supported_languages
            component_results['language_detection'] = ComponentResult(
                name="language_detector",
                status=PipelineStatus.SKIPPED,
                data={"language": language, "confidence": language_confidence, "supported": language_supported},
                processing_time=time.time() - lang_detection_time
            )
            print(f"   ‚Üí Using extension detection: {language}")

        language = component_results['language_detection'].data.get("language", "unknown")
        language_supported = component_results['language_detection'].data.get("supported", False)
        language_confidence = component_results['language_detection'].data.get("confidence", 0.0)

        # Stop if language not supported OR confidence is too low
        if not language_supported or language == "unknown":
            reason = ""
            if language_confidence < self.language_confidence_threshold:
                reason = f"Low confidence detection ({language_confidence:.1%} < {self.language_confidence_threshold:.0%})"
            else:
                reason = f"Unsupported language: {language}"
                
            result = PipelineResult(
                filename=filename,
                language=language,
                language_detection=component_results['language_detection'],
                style_analysis=ComponentResult(
                    name="style_checker",
                    status=PipelineStatus.SKIPPED,
                    data={"passed": False, "score": 0, "error": reason},
                    error=reason
                ),
                security_analysis=ComponentResult(
                    name="security_scanner",
                    status=PipelineStatus.SKIPPED,
                    data={"passed": False, "score": 0, "error": reason},
                    error=reason
                ),
                overall_score=0,
                overall_status="UNSUPPORTED_LANGUAGE",
                final_decision="REJECT",
                total_processing_time=time.time() - start_time,
                timestamp=start_time,
                recommendations=[f"Language detection failed: {reason}"]
            )
            print(f"\n‚ùå Analysis stopped: {reason}")
            return result

        # ================== STEP 2: STYLE ANALYSIS ==================
        print(f"\n[2/5] üìã Style Analysis")
        style_analysis_time = time.time()

        if self.components['style_checker']:
            try:
                checker = self.components['style_checker']
                style_result = checker.analyze_code(code, filename)

                # Convert to dict if needed
                if hasattr(style_result, 'to_dict'):
                    style_data = style_result.to_dict()
                else:
                    style_data = style_result

                component_results['style_analysis'] = ComponentResult(
                    name="style_checker",
                    status=PipelineStatus.SUCCESS,
                    data=style_data,
                    processing_time=time.time() - style_analysis_time
                )

                print(f"   ‚Üí Score: {style_data.get('score', 0)}/100")
                print(f"   ‚Üí Errors: {len(style_data.get('errors', []))}, Warnings: {len(style_data.get('warnings', []))}")

                # Add recommendations
                style_score = style_data.get('score', 100)
                if style_score < 70:
                    recommendations.append(f"Improve code style (score: {style_score}/100)")

            except Exception as e:
                component_results['style_analysis'] = ComponentResult(
                    name="style_checker",
                    status=PipelineStatus.ERROR,
                    data={"passed": False, "score": 0, "error": str(e)},
                    error=str(e),
                    processing_time=time.time() - style_analysis_time
                )
                print(f"   ‚Üí Failed: {e}")
        else:
            component_results['style_analysis'] = ComponentResult(
                name="style_checker",
                status=PipelineStatus.SKIPPED,
                data={"passed": True, "score": 80, "skip_reason": "Component unavailable"},
                processing_time=time.time() - style_analysis_time
            )
            print(f"   ‚Üí Skipped (component unavailable)")

        # ================== STEP 3: SECURITY ANALYSIS ==================
        print(f"\n[3/5] üîí Security Analysis")
        security_analysis_time = time.time()

        if self.components['security_scanner']:
            try:
                scanner = self.components['security_scanner']
                security_result = scanner.scan(language, code, filename)

                component_results['security_analysis'] = ComponentResult(
                    name="security_scanner",
                    status=PipelineStatus.SUCCESS,
                    data=security_result,
                    processing_time=time.time() - security_analysis_time
                )

                print(f"   ‚Üí Score: {security_result.get('score', 0)}/100")
                print(f"   ‚Üí Risk Level: {security_result.get('risk_level', 'UNKNOWN')}")
                print(f"   ‚Üí Findings: {len(security_result.get('findings', []))}")

                # Add recommendations
                security_score = security_result.get('score', 100)
                if security_score < 80:
                    recommendations.append(f"Address security issues (score: {security_score}/100)")

                findings = security_result.get('findings', [])
                critical_findings = [f for f in findings if f.get('severity') in ['CRITICAL', 'HIGH']]
                if critical_findings:
                    recommendations.append(f"Fix {len(critical_findings)} critical/high security findings")

            except Exception as e:
                component_results['security_analysis'] = ComponentResult(
                    name="security_scanner",
                    status=PipelineStatus.ERROR,
                    data={"passed": False, "score": 0, "error": str(e)},
                    error=str(e),
                    processing_time=time.time() - security_analysis_time
                )
                print(f"   ‚Üí Failed: {e}")
        else:
            component_results['security_analysis'] = ComponentResult(
                name="security_scanner",
                status=PipelineStatus.SKIPPED,
                data={"passed": True, "score": 80, "skip_reason": "Component unavailable"},
                processing_time=time.time() - security_analysis_time
            )
            print(f"   ‚Üí Skipped (component unavailable)")

        # ================== STEP 4: AUG-PDG ANALYSIS ==================
        print(f"\n[4/5] üèóÔ∏è AUG-PDG Analysis")
        aug_pdg_time = time.time()

        if self.components['aug_pdg_builder']:
            try:
                pdg_pipeline = self.components['aug_pdg_builder'](language=language)
                pdg_result = pdg_pipeline.run(code)

                pdg_stats = pdg_result.get('stats', {})
                pdg_vulnerabilities = pdg_result.get('vulnerabilities', [])

                component_results['aug_pdg_analysis'] = ComponentResult(
                    name="aug_pdg",
                    status=PipelineStatus.SUCCESS,
                    data={
                        "pdg_nodes": pdg_stats.get('pdg_nodes', 0),
                        "pdg_edges": pdg_stats.get('pdg_edges', 0),
                        "security_issues": pdg_stats.get('security_issues', 0),
                        "vulnerabilities": pdg_vulnerabilities
                    },
                    processing_time=time.time() - aug_pdg_time
                )

                print(f"   ‚Üí PDG Nodes: {pdg_stats.get('pdg_nodes', 0)}")
                print(f"   ‚Üí PDG Edges: {pdg_stats.get('pdg_edges', 0)}")
                print(f"   ‚Üí Security Issues: {pdg_stats.get('security_issues', 0)}")

                # Extract vulnerability patterns for model
                vulnerability_patterns = []
                for vuln in pdg_vulnerabilities[:3]:  # Limit to top 3
                    if 'variables_used' in vuln and 'variables_defined' in vuln:
                        source = ", ".join(vuln.get('variables_used', [])) or "user_input"
                        sink = ", ".join(vuln.get('variables_defined', [])) or "dangerous_operation"
                        vulnerability_patterns.append({
                            'source': source,
                            'sink': sink,
                            'sanitization': 'none' if vuln.get('involves_tainted_input', False) else 'unknown'
                        })

                component_results['aug_pdg_analysis'].data['vulnerability_patterns'] = vulnerability_patterns

                if pdg_vulnerabilities:
                    recommendations.append(f"Review {len(pdg_vulnerabilities)} PDG-identified vulnerabilities")

            except Exception as e:
                component_results['aug_pdg_analysis'] = ComponentResult(
                    name="aug_pdg",
                    status=PipelineStatus.ERROR,
                    data={
                        "pdg_nodes": 0,
                        "pdg_edges": 0,
                        "vulnerabilities": [],
                        "error": str(e)
                    },
                    error=str(e),
                    processing_time=time.time() - aug_pdg_time
                )
                print(f"   ‚Üí Failed: {e}")
        else:
            component_results['aug_pdg_analysis'] = ComponentResult(
                name="aug_pdg",
                status=PipelineStatus.SKIPPED,
                data={"pdg_nodes": 0, "pdg_edges": 0, "vulnerabilities": [], "skip_reason": "Component unavailable"},
                processing_time=time.time() - aug_pdg_time
            )
            print(f"   ‚Üí Skipped (component unavailable)")

        # ================== STEP 5: VULNERABILITY MODEL PREDICTION ==================
        print(f"\n[5/5] ü§ñ Vulnerability Model Prediction")
        vulnerability_time = time.time()

        try:
            # Extract vulnerability patterns from AUG-PDG or use security findings
            vulnerability_patterns = []

            # Try to get from AUG-PDG
            if component_results.get('aug_pdg_analysis') and \
               component_results['aug_pdg_analysis'].status == PipelineStatus.SUCCESS:
                patterns = component_results['aug_pdg_analysis'].data.get('vulnerability_patterns', [])
                if patterns:
                    vulnerability_patterns = patterns

            # If no patterns from AUG-PDG, create from security findings
            if not vulnerability_patterns and \
               component_results['security_analysis'].status == PipelineStatus.SUCCESS:
                security_data = component_results['security_analysis'].data
                findings = security_data.get('findings', [])
                for finding in findings[:2]:  # Limit to top 2
                    vulnerability_patterns.append({
                        'source': 'user_input',
                        'sink': finding.get('category', 'dangerous_operation'),
                        'sanitization': 'none' if 'injection' in finding.get('category', '').lower() else 'unknown'
                    })

            # Fallback: create simple patterns from common sources/sinks
            if not vulnerability_patterns:
                vulnerability_patterns = [
                    {'source': 'user_input', 'sink': 'system_call', 'sanitization': 'unknown'},
                    {'source': 'user_input', 'sink': 'database_query', 'sanitization': 'unknown'}
                ]

            predictions = []
            for pattern in vulnerability_patterns[:2]:  # Limit to 2 predictions
                pred = self.vulnerability_model.predict_vulnerability(
                    pattern['source'],
                    pattern['sink'],
                    pattern['sanitization']
                )
                pred['pattern'] = pattern
                predictions.append(pred)

            component_results['vulnerability_prediction'] = ComponentResult(
                name="vulnerability_model",
                status=PipelineStatus.SUCCESS,
                data={
                    "predictions": predictions,
                    "total_predictions": len(predictions),
                    "vulnerable_count": sum(1 for p in predictions if p.get('is_vulnerable', False)),
                    "model_status": self.vulnerability_model.get_status()
                },
                processing_time=time.time() - vulnerability_time
            )

            print(f"   ‚Üí Predictions: {len(predictions)}")
            print(f"   ‚Üí Vulnerable predictions: {sum(1 for p in predictions if p.get('is_vulnerable', False))}")

            # Add recommendations based on model predictions
            vulnerable_predictions = [p for p in predictions if p.get('is_vulnerable', False)]
            if vulnerable_predictions:
                recommendations.append(f"Model detected {len(vulnerable_predictions)} vulnerable patterns")

        except Exception as e:
            component_results['vulnerability_prediction'] = ComponentResult(
                name="vulnerability_model",
                status=PipelineStatus.ERROR,
                data={
                    "predictions": [], 
                    "error": str(e),
                    "model_status": self.vulnerability_model.get_status()
                },
                error=str(e),
                processing_time=time.time() - vulnerability_time
            )
            print(f"   ‚Üí Failed: {e}")

        # ================== FINAL DECISION ==================
        total_time = time.time() - start_time

        # Calculate overall score
        style_score = component_results['style_analysis'].data.get('score', 100)
        security_score = component_results['security_analysis'].data.get('score', 100)

        # Weighted average
        overall_score = int((style_score * 0.4) + (security_score * 0.6))

        # Get vulnerability data safely
        vulnerability_data = {}
        if 'vulnerability_prediction' in component_results:
            vuln_result = component_results['vulnerability_prediction']
            if vuln_result and hasattr(vuln_result, 'data'):
                vulnerability_data = vuln_result.data
            else:
                vulnerability_data = {}

        # Make final decision
        final_decision = self._make_decision(
            overall_score,
            component_results['security_analysis'].data,
            vulnerability_data
        )

        # Determine overall status
        if overall_score >= 90:
            overall_status = "EXCELLENT"
        elif overall_score >= 75:
            overall_status = "GOOD"
        elif overall_score >= 60:
            overall_status = "FAIR"
        else:
            overall_status = "POOR"

        # Create final result
        result = PipelineResult(
            filename=filename,
            language=language,
            language_detection=component_results['language_detection'],
            style_analysis=component_results['style_analysis'],
            security_analysis=component_results['security_analysis'],
            aug_pdg_analysis=component_results.get('aug_pdg_analysis'),
            vulnerability_prediction=component_results.get('vulnerability_prediction'),
            overall_score=overall_score,
            overall_status=overall_status,
            final_decision=final_decision,
            total_processing_time=total_time,
            timestamp=start_time,
            recommendations=recommendations
        )

        print(f"\n{'='*70}")
        print(f"‚úÖ ANALYSIS COMPLETE")
        print(f"{'='*70}")
        print(f"üìä Overall Score: {overall_score}/100 ({overall_status})")
        print(f"‚öñÔ∏è  Final Decision: {final_decision}")
        print(f"‚è±Ô∏è  Total Time: {total_time:.2f}s")
        print(f"üí° Recommendations: {len(recommendations)}")
        print(f"ü§ñ Model Status: {self.vulnerability_model.get_status()['status']}")
        print(f"{'='*70}")

        return result

    def _make_decision(self, overall_score, security_data, vulnerability_data):
        """
        Make final decision on code safety
        """
        security_score = security_data.get('score', 100)
        risk_level = security_data.get('risk_level', 'LOW')

        # Check vulnerability model predictions
        vulnerable_count = vulnerability_data.get('vulnerable_count', 0)

        # Decision logic
        if security_score < 60 or risk_level in ['CRITICAL', 'HIGH']:
            return "REJECT"
        elif vulnerable_count > 0:
            return "REVIEW_REQUIRED"
        elif security_score < 75:
            return "REVIEW_RECOMMENDED"
        else:
            return "APPROVE"

    def _detect_by_extension(self, filename: str) -> str:
        """Detect language by file extension."""
        ext_map = {
            '.py': 'python',
            '.js': 'javascript',
            '.java': 'java',
            '.php': 'php',
            '.rb': 'ruby',
            '.go': 'go',
        }

        for ext, lang in ext_map.items():
            if filename.lower().endswith(ext):
                return lang

        return "unknown"

    def get_model_status(self):
        """Get vulnerability model status."""
        return self.vulnerability_model.get_status()

# ================== MAIN EXECUTION ==================

def main():
    """Main function to run the complete pipeline"""

    print("=" * 80)
    print("üöÄ CODE REVIEW ASSISTANT - COMPLETE PIPELINE")
    print("=" * 80)
    print("Workflow: File ‚Üí Language Detector ‚Üí Style Check ‚Üí Security Scan ‚Üí")
    print("          AUG-PDG ‚Üí Vulnerability Model ‚Üí Final Decision")
    print("=" * 80)

    try:
        # Initialize pipeline (will raise ModelLoadError if model missing)
        pipeline = CodeReviewPipeline()
        
        # Show model status
        model_status = pipeline.get_model_status()
        print(f"\nü§ñ Model Status: {model_status['status']}")
        print(f"üìÅ Model Path: {model_status['model_path']}")
        print(f"‚ö° Device: {model_status['device']}")
        
    except ModelLoadError as e:
        print(f"\n‚ùå CRITICAL ERROR: {e}")
        print(f"\nüí° SOLUTION: Run this command first:")
        print(f"   python scripts/download_model.py")
        return 1

    # Ask for file path
    file_path = input("\nüìÇ Enter path to code file: ").strip()

    if not file_path:
        print("‚ö†Ô∏è No file path provided. Using example file...")

        # Create example file
        example_code = '''import os
import subprocess

def calculate_sum(a, b):
    """Calculate sum of two numbers."""
    return a + b

def vulnerable_function(user_input):
    # Simulated vulnerability
    os.system("echo " + user_input)  # Command injection
    return "Done"

def main():
    x = 5
    y = 10
    result = calculate_sum(x, y)
    print(f"Sum: {result}")

    # Test vulnerable function
    vulnerable_function("test")

if __name__ == "__main__":
    main()'''

        file_path = "/tmp/example.py"
        with open(file_path, 'w') as f:
            f.write(example_code)
        print(f"Created example file at: {file_path}")

    try:
        # Process the file
        result = pipeline.process_file(file_path)

        # Generate report
        print("\n" + "=" * 80)
        print("üìã FINAL ANALYSIS REPORT")
        print("=" * 80)

        # Language info
        print(f"\nüî§ Language: {result.language}")
        print(f"üìÅ File: {result.filename}")

        # Style analysis
        style_data = result.style_analysis.data
        print(f"\nüìã STYLE ANALYSIS:")
        print(f"  Score: {style_data.get('score', 0)}/100")
        print(f"  Status: {'‚úÖ PASSED' if style_data.get('passed', False) else '‚ùå FAILED'}")
        if style_data.get('errors'):
            print(f"  Errors: {len(style_data.get('errors', []))}")

        # Security analysis
        security_data = result.security_analysis.data
        print(f"\nüîí SECURITY ANALYSIS:")
        print(f"  Score: {security_data.get('score', 0)}/100")
        print(f"  Risk Level: {security_data.get('risk_level', 'UNKNOWN')}")
        print(f"  Findings: {len(security_data.get('findings', []))}")

        # AUG-PDG analysis
        if result.aug_pdg_analysis:
            pdg_data = result.aug_pdg_analysis.data
            print(f"\nüèóÔ∏è AUG-PDG ANALYSIS:")
            print(f"  PDG Nodes: {pdg_data.get('pdg_nodes', 0)}")
            print(f"  PDG Edges: {pdg_data.get('pdg_edges', 0)}")
            print(f"  Vulnerabilities: {len(pdg_data.get('vulnerabilities', []))}")

        # Vulnerability model predictions
        if result.vulnerability_prediction:
            vuln_data = result.vulnerability_prediction.data
            print(f"\nü§ñ VULNERABILITY MODEL:")
            print(f"  Status: {vuln_data.get('model_status', {}).get('status', 'UNKNOWN')}")
            print(f"  Predictions: {vuln_data.get('total_predictions', 0)}")
            print(f"  Vulnerable: {vuln_data.get('vulnerable_count', 0)}")
            
            # Show individual predictions
            predictions = vuln_data.get('predictions', [])
            for i, pred in enumerate(predictions, 1):
                print(f"    {i}. Source: {pred.get('pattern', {}).get('source', 'N/A')}")
                print(f"       Sink: {pred.get('pattern', {}).get('sink', 'N/A')}")
                print(f"       Vulnerable: {'‚úÖ Yes' if pred.get('is_vulnerable') else '‚ùå No'} (Confidence: {pred.get('confidence', 0):.1%})")

        # Final decision
        print(f"\n" + "=" * 80)
        print(f"‚öñÔ∏è FINAL DECISION: {result.final_decision}")
        print("=" * 80)

        if result.final_decision == "APPROVE":
            print("‚úÖ Code is SAFE to merge")
        elif result.final_decision == "REVIEW_REQUIRED":
            print("‚ö†Ô∏è Code requires SECURITY REVIEW before merging")
        elif result.final_decision == "REVIEW_RECOMMENDED":
            print("‚ö†Ô∏è Code review recommended")
        else:
            print("‚ùå Code is NOT SAFE - REJECT pull request")

        # Recommendations
        if result.recommendations:
            print(f"\nüí° RECOMMENDATIONS:")
            for i, rec in enumerate(result.recommendations, 1):
                print(f"  {i}. {rec}")

        print(f"\n‚è±Ô∏è Total processing time: {result.total_processing_time:.2f}s")

    except Exception as e:
        print(f"\n‚ùå Error processing file: {e}")
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)
